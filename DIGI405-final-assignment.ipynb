{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51655f1c-23d9-45d0-9dd9-16afa391f778",
   "metadata": {},
   "source": [
    "# DIGI405 Text Analysis Project Notebook\n",
    "\n",
    "Version 0.2\n",
    "\n",
    "You should use this notebook as a starting point for your DIGI405 project. It provides code to select your dataset, and run a complete text classification pipeline with [textplumber](https://geoffford.nz/textplumber/), a package that provides an easy to use interface to methods covered in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc913ba6-fcd5-4dec-af85-2558d4bcab2a",
   "metadata": {},
   "source": [
    "**Name:**  \n",
    "**Student ID:**  \n",
    "**Project option:** (ONE of 'Essay' or 'Sentiment' or 'Genre')  \n",
    "**Project submission date:**  \n",
    "\n",
    "Please also add your name to your notebook filename (where it says 'NAME')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7930af-0689-46ff-82e9-9422cffe4073",
   "metadata": {},
   "source": [
    "### Notebook structure\n",
    "\n",
    "Sections 1-4 provide code you should modify or extend. In your report, you can refer to code sections by their section number, eg 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b31b2a",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575fef0-63ff-49e1-9df1-9c590e202793",
   "metadata": {},
   "source": [
    "You must select the Python 3.12 kernel to run the code in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87237ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from textplumber.core import *\n",
    "from textplumber.clean import *\n",
    "from textplumber.preprocess import *\n",
    "from textplumber.tokens import *\n",
    "from textplumber.pos import *\n",
    "from textplumber.embeddings import *\n",
    "from textplumber.report import *\n",
    "from textplumber.store import *\n",
    "from textplumber.lexicons import *\n",
    "from textplumber.textstats import *\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "import warnings\n",
    "\n",
    "# in the interests of readability, ignoring this warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Your stop_words may be inconsistent with your preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9588bdd",
   "metadata": {},
   "source": [
    "These settings control the display of Pandas dataframes in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # show all columns\n",
    "pd.set_option('display.max_colwidth', 500) # increase this to see more text in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d47a58",
   "metadata": {},
   "source": [
    "Get word lists: \n",
    "* The stop word list is from NLTK.   \n",
    "* All of the word lists (including the stop word list) can be used to extract lexicon count features to extract features based on a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce00426",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words()\n",
    "stop_words_lexicon = {'stop_words': stop_words}\n",
    "empath_lexicons = get_empath_lexicons()\n",
    "vader_lexicons = get_sentiment_lexicons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e1848-5064-46dc-b8fb-a90e41ead2c5",
   "metadata": {},
   "source": [
    "## 2. Load and inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a182a",
   "metadata": {},
   "source": [
    "### 2.1 Choose a dataset and preview the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd33c2",
   "metadata": {},
   "source": [
    "Below you can select a dataset for the assignment. The options are `sentiment`, `essay` and `genre`. Change the value of `dataset_option` below. The datasets available on Huggingface.co will be downloaded automatically and a link provided to the dataset card with more information. The `genre` dataset was distributed with this notebook.   \n",
    "\n",
    "Note:  The `movie_reviews` dataset is being used to demonstrate the notebook and is not one of your options for the assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 'essay', 'sentiment', or 'genre' ('movie_reviews' is just for testing/demonstration)\n",
    "dataset_option = 'movie_reviews' \n",
    "\n",
    "if dataset_option == 'movie_reviews':\n",
    "\tdataset_name = 'polsci/sentiment-polarity-dataset-v2.0'\n",
    "\tdataset_dir = None\n",
    "\ttarget_labels = ['neg', 'pos']\n",
    "\ttext_column = 'text'\n",
    "\tlabel_column = 'label'\n",
    "\ttrain_split_name = 'train'\n",
    "\ttest_split_name = 'train'\n",
    "\tprint('The movie_reviews is to demonstrate the notebook and is not an assignment option.')\n",
    "elif dataset_option == 'sentiment':\n",
    "\tdataset_name = 'cardiffnlp/tweet_eval'\n",
    "\tdataset_dir = 'sentiment'\n",
    "\ttarget_labels = ['negative', 'neutral', 'positive']\n",
    "\ttext_column = 'text'\n",
    "\tlabel_column = 'label'\n",
    "\ttrain_split_name = 'train'\n",
    "\ttest_split_name = 'validation'\n",
    "\tprint('You selected the sentiment dataset. Read more about this at https://huggingface.co/datasets/cardiffnlp/tweet_eval')\n",
    "elif dataset_option == 'essay':\n",
    "\tdataset_name = 'polsci/ghostbuster-essay-cleaned'\n",
    "\tdataset_dir = None\n",
    "\ttarget_labels = ['claude', 'gpt', 'human']\n",
    "\ttext_column = 'text'\n",
    "\tlabel_column = 'label'\n",
    "\ttrain_split_name = 'train'\n",
    "\ttest_split_name = 'test'\n",
    "\tprint('You selected the essay dataset. Read more about this at https://huggingface.co/datasets/polsci/ghostbuster-essay-cleaned')\n",
    "elif dataset_option == 'genre':\n",
    "    dataset_name = 'genre'\n",
    "    dataset_type = 'json'\n",
    "    dataset_dir = 'genre_dataset.json'\n",
    "    target_labels = ['Fiction', 'Letter', 'Notice', 'Obituary', 'Poetry or verse', 'Recipe', 'Review']\n",
    "    text_column = 'text'\n",
    "    label_column = 'label'\n",
    "    train_split_name = 'train'\n",
    "    test_split_name = 'test'\n",
    "    print('You selected the genre dataset.')\n",
    "else:\n",
    "\tprint('Try again! That was not an option!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04f51e",
   "metadata": {},
   "source": [
    "#### Important notes about specific datasets:\n",
    "\n",
    "* Make sure you go to the relevant Huggingface page to read more about the [essay](https://huggingface.co/datasets/polsci/ghostbuster-essay-cleaned) and [sentiment](https://huggingface.co/datasets/cardiffnlp/tweet_eval/viewer/sentiment) datasets. Note the sentiment dataset is one subset of the larger 'tweet_eval' dataset.  \n",
    "* For the *sentiment* dataset, it is challenging to get good accuracy with three classes. If you like you can remove the `neutral` class. There is a cell below that does this for you - don't change the cell above.\n",
    "* For the *essay* dataset, there are differences in punctuation between classes. You should use `character_replacements = {\"’\": \"'\", '“': '\"', '”': '\"',}` in the `TextCleaner` component in your pipeline to make sure you are not overfitting to a quirk of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2541b71",
   "metadata": {},
   "source": [
    "This loads the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39078bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_option != 'genre': # if loading from huggingface ...\n",
    "    dataset = load_dataset(dataset_name, data_dir=dataset_dir)\n",
    "else: # if loading the genre dataset from the provided json file\n",
    "    dataset = load_dataset(dataset_type, data_files=dataset_dir)\n",
    "    train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "    test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdffe84",
   "metadata": {},
   "source": [
    "This cell will show you information on the dataset fields and the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a48d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07bd82",
   "metadata": {},
   "source": [
    "Here is the breakdown of the composition of labels in each data-set split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting label column to ClassLabel if not already\n",
    "cast_column_to_label(dataset, label_column)\n",
    "label_names = get_label_names(dataset, label_column)\n",
    "\n",
    "dfs = {}\n",
    "for split in dataset.keys():\n",
    "    dfs[split] = dataset[split].to_pandas()\n",
    "    dfs[split].insert(1, 'label_name', dfs[split][label_column].apply(lambda x: dataset[split].features[label_column].int2str(x)))\n",
    "    print('Labels for {}:'.format(split))\n",
    "    preview_label_counts(dfs[split], label_column, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d36f79",
   "metadata": {},
   "source": [
    "### 2.2 Configure the labels (optional)\n",
    "\n",
    "* You can override the default labels for the data-set here to make the task more or less challenging. High accuracy does not guarantee a high grade. \n",
    "* See the assignment instructions and the dataset card or corresponding paper for explanations of the data.  \n",
    "* Read the comments below and uncomment the relevant lines for your data-set if and amend the label names if needed.\n",
    "* Remember, this is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f500de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the movie reviews dataset (this is just for testing/demonstration) - there are 2 labels and that is it!\n",
    "\n",
    "# for the sentiment dataset - there are 3 labels - you can make the task simpler as a binary classification problem using one of these options:\n",
    "#target_labels = ['negative', 'neutral']\n",
    "#target_labels = ['negative', 'positive']\n",
    "#target_labels = ['neutral', 'positive']\n",
    "\n",
    "# for the essay dataset - there are 7 labels - you can make the task simpler as a binary classification problem using one of these options:\n",
    "#target_labels = ['claude', 'gpt']\n",
    "#target_labels = ['human', 'gpt'] \n",
    "#target_labels = ['human', 'claude']\n",
    "\n",
    "# for the genre dataset - there are 7 labels - you can turn the task into one or more binary classification problems using options such as:\n",
    "#target_labels = ['Letter', 'Notice']\n",
    "#target_labels = ['Letter', 'Fiction']\n",
    "#target_labels = ['Review', 'Fiction']\n",
    "#target_labels = ['Notice', 'Obituary']\n",
    "\n",
    "print(target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a7d92",
   "metadata": {},
   "source": [
    "### 2.3 Prepare the train and test splits\n",
    "\n",
    "* This cell handles the train-test split for you.\n",
    "* Some of the data-sets are unbalanced. This cell will balance the training data using under-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ff72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [label_names.index(name) for name in target_labels]\n",
    "target_names = [label_names[i] for i in target_classes]\n",
    "\n",
    "if train_split_name == test_split_name:\n",
    "    X = dataset[train_split_name].to_pandas()\n",
    "    X.insert(1, 'label_name', dfs[train_split_name][label_column].apply(lambda x: dataset[train_split_name].features[label_column].int2str(x)))\n",
    "    y = np.array(dataset[train_split_name][label_column])\n",
    "\n",
    "    mask = np.isin(y, target_classes)\n",
    "    X = X.loc[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # creating df splits with original data first  - so can look at the train data if needed\n",
    "    dfs['train'], dfs['test'], y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # we're just using the text for features\n",
    "    X_train = np.array(dfs['train'][text_column])\n",
    "    X_test = np.array(dfs['test'][text_column])\n",
    "else:\n",
    "    X_train = np.array(dataset[train_split_name][text_column])\n",
    "    y_train = np.array(dataset[train_split_name][label_column])\n",
    "    X_test = np.array(dataset[test_split_name][text_column])\n",
    "    y_test = np.array(dataset[test_split_name][label_column])\n",
    "\n",
    "    mask = np.isin(y_train, target_classes)\n",
    "    mask_test = np.isin(y_test, target_classes)\n",
    "\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    X_test = X_test[mask_test]\n",
    "    y_test = y_test[mask_test]\n",
    "\n",
    "# this cell undersamples all but the minority class to balance the training data\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_train, y_train)\n",
    "X_train = X_train.reshape(-1)\n",
    "\n",
    "preview_splits(X_train, y_train, X_test, y_test, target_classes = target_classes, target_names = target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d3de0",
   "metadata": {},
   "source": [
    "### 2.4 Preview the texts\n",
    "\n",
    "Time to get to know your data. We will only preview the train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_names = map(lambda x: label_names[x], y_train)\n",
    "display(dfs['train'][dfs['train']['label_name'].isin(y_train_names)].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8b2b0",
   "metadata": {},
   "source": [
    "Enter the index (the number in the first column) as `selected_index` to see the row. The `limit` value controls how much of the text you see. Set a higher limit to see more of the text or set it to 0 to see all of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583aef-080f-427a-b883-6a7c7f633dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the full text of a selected article by dataframe index\n",
    "selected_index = 10\n",
    "\n",
    "preview_row_text(dfs['train'], selected_index, text_column = text_column, limit=400) # change limit to see more of the text if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca47acd",
   "metadata": {},
   "source": [
    "## 3. Create a classification pipeline and train a model\n",
    "\n",
    "Create a Sci-kit Learn pipeline to preprocess the texts and train a classification model. The pipeline components will be added in through the notebook. There are a number of pipeline components you can access through the `textplumber` package. You will have an opportunity to learn about this in labs, but documentation is [available here](https://geoffford.nz/textplumber)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe248aaa",
   "metadata": {},
   "source": [
    "To speed up preprocessing some of the pipeline components store the preprocessed data in a cache to avoid recomputing them. Run this as is - it will create an SQLite file with the name of your dataset option in the directory of the notebook. This will speed up some repeated processing (e.g. tokenization with Spacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285160e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = TextFeatureStore(f'assignment-{dataset_option}.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a558f6d",
   "metadata": {},
   "source": [
    "The pipeline below includes a number of different components. Most are commented out on the first run of the notebook. There are lots of options for each component. You will need to look at the documentation and examples in labs to learn about these. These components can extract different kinds of features, any of which can be applied to build a model. The feature types include:\n",
    "\n",
    "* Token features  \n",
    "* Bigram features  \n",
    "* Parts of speech features\n",
    "* Lexicon-based features  \n",
    "* Document-level statistics  \n",
    "* Text embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0567b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "\t('cleaner', TextCleaner(strip_whitespace=True)), # for the essay dataset you should use character_replacements = {\"’\": \"'\", '“': '\"', '”': '\"',}\n",
    "\t('spacy', SpacyPreprocessor(feature_store=feature_store)),\n",
    "\t('features', FeatureUnion([\n",
    "\t\t('tokens', # token features - these can be single tokens or ngrams of tokens using TokensVectorizer - see textplumber documentation for examples\n",
    "\t\t\tPipeline([\n",
    "\t\t\t\t('spacy_token_vectorizer', TokensVectorizer(feature_store = feature_store, vectorizer_type='count', max_features=100, lowercase = True, remove_punctuation = True, stop_words = stop_words, min_df=0.0, max_df=1.0, ngram_range=(1, 1))),\n",
    "\t\t\t\t# ('selector', SelectKBest(score_func=mutual_info_classif, k=100)), # uncomment for feature selection\n",
    "\t\t\t\t# ('scaler', StandardScaler(with_mean=False)),\n",
    "\t\t\t\t], verbose = True)),\n",
    "\n",
    "\t\t# ('pos', # pos features - these can be a single label or ngrams of pos tags using POSVectorizer - see textplumber documentation for examples\n",
    "\t\t# \tPipeline([\n",
    "\t\t# \t\t('spacy_pos_vectorizer', POSVectorizer(feature_store=feature_store)),\n",
    "\t\t# \t\t#('selector', SelectKBest(score_func=mutual_info_classif, k=5)),\n",
    "\t\t# \t\t('scaler', StandardScaler(with_mean=False)),\n",
    "\t\t# \t\t], verbose = True)),\n",
    "\n",
    "\t\t#('textstats', # document-level text statistics using TextstatsTransformer - see textplumber documentation for examples\n",
    "\t\t# \tPipeline([\n",
    "\t\t# \t\t('textstats_vectorizer', TextstatsTransformer(feature_store=feature_store)),\n",
    "\t\t# \t\t('scaler', StandardScaler(with_mean=False)),\n",
    "\t\t# \t\t], verbose = True)),\n",
    "\n",
    "\t\t# ('lexicon', # lexicon features - defined above are empath_lexicons, sentiment_lexicons and stop_words_lexicon - see textplumber documentation for examples\n",
    "\t\t# \tPipeline([\n",
    "\t\t# \t\t('lexicon_vectorizer', LexiconCountVectorizer(feature_store=feature_store, lexicons=empath_lexicons)), # the notebook has already provided example lexicons right at the top!\n",
    "\t\t#  \t\t#('selector', SelectKBest(score_func=mutual_info_classif, k=5)),\n",
    "\t\t# \t\t('scaler', StandardScaler(with_mean=False)),\n",
    "\t\t# \t\t], verbose = True)),\n",
    "\n",
    "\t\t# ('embeddings', Model2VecEmbedder(feature_store=feature_store)), # extract embeddings using Model2Vec - textplumber documentation for examples\n",
    "\n",
    "\t\t], verbose = True)),\n",
    "\t\n",
    "\t('classifier', LogisticRegression(max_iter=5000, random_state=42)) # for logistic regression - only select one classifier!\n",
    "    #('classifier', DecisionTreeClassifier(max_depth = 3, random_state=42)) # for decision tree - only select one classifier!\n",
    "], verbose = True) # using verbose because I like to see what is going on\n",
    "\n",
    "display(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f970c0",
   "metadata": {},
   "source": [
    "Run the predictions and output model metrics and a confusion matrix using this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_predicted, target_names = target_names, digits=3, zero_division=0))\n",
    "plot_confusion_matrix(y_test, y_predicted, target_classes, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accca8ed",
   "metadata": {},
   "source": [
    "The cell below is commented out, but you have the option to uncomment it to run a grid search based on the pipeline you've created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c76909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: if you get a warning about tokenizers and parallelism - uncomment this line \n",
    "# # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # setup gridsearch to test different max_features\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {\n",
    "#     'features__tokens__spacy_token_vectorizer__max_features': [50, 100, 150, 200, 250, 300],  # this assumes you are using the tokens part of the pipeline\n",
    "#     # 'features__tokens__selector__k': [50, 100, 150, 200, 250, 300],  # this assumes you have enabled the selector for tokens\n",
    "# }\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_macro', verbose=100, n_jobs=1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print('\\n-----------------------------------------------------------------')\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best score found: \", grid_search.best_score_)\n",
    "# print('-----------------------------------------------------------------\\n')\n",
    "\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_pred, target_names = target_names, digits=3))\n",
    "# plot_confusion_matrix(y_test, y_pred, target_classes, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab181500",
   "metadata": {},
   "source": [
    "## 4. Evaluate your model and investigate model predictions\n",
    "\n",
    "You already have some metrics in the cell above. Below is some additional reporting to help you understand your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bddd74",
   "metadata": {},
   "source": [
    "### 4.1 Classifier-specific features\n",
    "\n",
    "If you are using a Decision Tree classifier in your pipeline, this will plot it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d02568",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline.named_steps['classifier'].__class__.__name__ == 'DecisionTreeClassifier':\n",
    "    plot_decision_tree_from_pipeline(pipeline, X_train, y_train, target_classes, target_names, 'classifier', 'features')\n",
    "else:\n",
    "    print('The classifier is not a decision tree - so no plot is shown!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16044bcb",
   "metadata": {},
   "source": [
    "If you are using a Logistic Regression classifier in your pipeline, this will plot the coefficients of the features in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline.named_steps['classifier'].__class__.__name__ == 'LogisticRegression':\n",
    "\tplot_logistic_regression_features_from_pipeline(pipeline, target_classes, target_names, top_n=20, classifier_step_name = 'classifier', features_step_name = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2e17b",
   "metadata": {},
   "source": [
    "### 4.2 Investigate correct and incorrect predictions\n",
    "\n",
    "To see the predictions of your model run this cell. The output can be quite long depending on the dataset and the number of misclassifications. The Pandas `max_rows` is configured at the top of the cell to restrict the length of output. You can adjust this as required. This is reset back to the Pandas default at the end of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b30f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust max rows\n",
    "pd.set_option('display.max_rows', 5) # show all rows\n",
    "\n",
    "# creating dataframe from y_predicted, y_test and the text\n",
    "predictions_df = pd.DataFrame(data = {'true': y_test, 'predicted': y_predicted})\n",
    "y_predicted_probs = pipeline.predict_proba(X_test)\n",
    "y_predicted_probs = np.round(y_predicted_probs, 3)\n",
    "columns = [f'{target_names[i]}_prob' for i in range(len(target_names))]\n",
    "predictions_df['predicted'] = predictions_df['predicted'].apply(lambda x: label_names[x])\n",
    "predictions_df['true'] = predictions_df['true'].apply(lambda x: label_names[x])\n",
    "predictions_df['correct'] = predictions_df['true'] == predictions_df['predicted']\n",
    "predictions_df['text'] = X_test\n",
    "predictions_df = pd.concat([predictions_df, pd.DataFrame(y_predicted_probs, columns=columns)], axis=1)\n",
    "\n",
    "# output a preview of docs for each cell of confusion matrix ...\n",
    "for true_target, target_name in enumerate(target_names):\n",
    "    for predicted_target, target_name in enumerate(target_names):\n",
    "        if true_target == predicted_target:\n",
    "            print(f'\\nCORRECTLY CLASSIFIED: {target_names[true_target]}')\n",
    "        else:\n",
    "            print(f'\\n{target_names[true_target]} INCORRECTLY CLASSIFIED as: {target_names[predicted_target]}')\n",
    "        print('=================================================================')\n",
    "\n",
    "        display(predictions_df[(predictions_df['true'] == target_names[true_target]) & (predictions_df['predicted'] == target_names[predicted_target])])\n",
    "\n",
    "pd.set_option('display.max_rows', 60) # setting back to the default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab796f98",
   "metadata": {},
   "source": [
    "### 4.3 Run inference on new (or old) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2120a",
   "metadata": {},
   "source": [
    "You can also run inference on new data (or any of the texts from training/validation) by changing the contents of the `texts` list below. This outputs a prediction, the probabilities of each class and the features present within the text that are used by the model to make its predictions. The numbers for each feature are the input to the final step of the pipeline. They may be scaled or transformed depending on the pipeline components you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['''\n",
    "It was excellent!\n",
    "''',\n",
    "\t\t'''\n",
    "This was a terrible movie!\n",
    "''',\n",
    "\t'''\n",
    "This might not not be the best movie ever made, or it could be the best movie of no time.\n",
    "''',\n",
    "]\n",
    "\n",
    "y_inference = pipeline.predict(texts)\n",
    "\n",
    "preprocessor = Pipeline(pipeline.steps[:-1])\n",
    "feature_names = preprocessor.named_steps['features'].get_feature_names_out()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "\tprint(f\"Text {i}: {text}\")\n",
    "\t\n",
    "\tprint(f\"\\tPredicted class: {label_names[y_inference[i]]}\")\n",
    "\tprint()\n",
    "\n",
    "\ty_inference_proba = pipeline.predict_proba([text])\n",
    "\tfor i, prob in enumerate(y_inference_proba[0]):\n",
    "\t\tprint(f\"\\tProbability of class {target_names[i]}: {prob:.2f}\")\n",
    "\n",
    "\tprint()\n",
    "\tprint(\"\\tFeatures:\")\n",
    "\n",
    "\tembeddings = 0\n",
    "    \n",
    "\tfrequencies = preprocessor.transform([text])\n",
    "\tif not isinstance(frequencies, np.ndarray):\n",
    "\t\tfrequencies = frequencies.toarray()\n",
    "\tfrequencies = frequencies[0].T\n",
    "    \n",
    "\tfor j, freq in enumerate(frequencies):\n",
    "\t\tif feature_names[j].startswith('embeddings_'):\n",
    "\t\t\tembeddings += 1\n",
    "\t\telif freq > 0:\n",
    "\t\t\tprint(f\"\\t{feature_names[j]}: {freq:.2f}\")\n",
    "\tif embeddings > 0:\n",
    "\t\tprint(f\"\\tFeatures also include {embeddings} embedding dimensions\")\n",
    "\n",
    "\tprint()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
